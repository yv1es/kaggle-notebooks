{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df): \n",
    "    dff = pd.DataFrame()\n",
    "    \n",
    "    # min max normalize column \n",
    "    normalize = lambda s : (s - s.min()) / (s.max() - s.min())\n",
    "\n",
    "    # split id into group and nr and normalize\n",
    "    dff['Group'] = pd.to_numeric(df['PassengerId'].str[:4]) \n",
    "    dff['Nr'] = pd.to_numeric(df['PassengerId'].str[5:])\n",
    "    dff['Group'] = normalize(dff['Group'])\n",
    "    dff['Nr'] = normalize(dff['Nr'])\n",
    "    \n",
    "    # one hot encode planet\n",
    "    dff = pd.concat([dff, pd.get_dummies(df['HomePlanet'])], axis=1)\n",
    "    \n",
    "    # encode CrypSleep bool as 0/1\n",
    "    dff['CryoSleep'] = df['CryoSleep'].map({False:0, True:1}).fillna(False).astype(np.uint8)\n",
    "\n",
    "    # Split cabin into Deck, Cabin, Side\n",
    "    dff = pd.concat([dff, df['Cabin'].str.split('/', expand=True)], axis=1)\n",
    "    dff.rename({0:'Deck', 1:'Cabin', 2:'Side'}, axis=1, inplace=True)\n",
    "    \n",
    "    # normalize cabin number\n",
    "    dff['Cabin'] = normalize(dff['Cabin'].astype(float).fillna(0))\n",
    "    \n",
    "    # one hot encode deck\n",
    "    dff = pd.concat([dff, pd.get_dummies(dff['Deck'], prefix='Deck')], axis=1)\n",
    "    dff.drop('Deck', axis=1, inplace=True)\n",
    "\n",
    "    # encode Side as P=0 S=1\n",
    "    dff['Side'] = dff['Side'].map({'P':0, 'S':1}).fillna(0.5).astype(np.float64)\n",
    "    \n",
    "    # one hot encode Destination \n",
    "    dff = pd.concat([dff, pd.get_dummies(df['Destination'], prefix='Destination')], axis=1)\n",
    "    \n",
    "    # normalize age \n",
    "    dff['Age'] = normalize(df['Age'].fillna('0').astype(int))\n",
    "    \n",
    "    # encode VIP bool as 0/1\n",
    "    dff['VIP'] = df['VIP'].map({False:0, True:1}).fillna(False).astype(np.uint8)\n",
    "\n",
    "    # nomalize\n",
    "    dff['RoomService'] = normalize(df['RoomService'].fillna('0').astype(float))\n",
    "    dff['FoodCourt'] = normalize(df['FoodCourt'].fillna('0').astype(float))\n",
    "    dff['Spa'] = normalize(df['Spa'].fillna('0').astype(float))\n",
    "    dff['VRDeck'] = normalize(df['VRDeck'].fillna('0').astype(float))\n",
    "     \n",
    "    return dff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess(df_train)\n",
    "y = df_train['Transported'].map({False:0, True:1}).fillna(False).astype(np.uint8)\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "# splitting into train and test data\n",
    "test_size = 0.1\n",
    "split_at = int(X.shape[0] * (1 - test_size))\n",
    "\n",
    "X_train = X.iloc[:split_at,:]\n",
    "X_val = X.iloc[split_at:,:]\n",
    "\n",
    "y_train = y.iloc[:split_at,:]\n",
    "y_val = y.iloc[split_at:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep(\n",
       "  (layer1): Linear(in_features=25, out_features=50, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (layer3): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act3): ReLU()\n",
       "  (layer4): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act4): ReLU()\n",
       "  (layer5): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act5): ReLU()\n",
       "  (layer6): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act6): ReLU()\n",
       "  (layer7): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act7): ReLU()\n",
       "  (layer8): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act8): ReLU()\n",
       "  (layer9): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (act9): ReLU()\n",
       "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(25, 50)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(50, 50)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(50, 50)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.layer5 = nn.Linear(50, 50)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.layer6 = nn.Linear(50, 50)\n",
    "        self.act6 = nn.ReLU()\n",
    "        self.layer7 = nn.Linear(50, 50)\n",
    "        self.act7 = nn.ReLU()\n",
    "        self.layer8 = nn.Linear(50, 50)\n",
    "        self.act8 = nn.ReLU()\n",
    "        self.layer9 = nn.Linear(50, 50)\n",
    "        self.act9 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.output = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.act4(self.layer4(x))\n",
    "        x = self.act5(self.layer5(x))\n",
    "        x = self.act6(self.layer6(x))\n",
    "        x = self.act7(self.layer7(x))\n",
    "        x = self.act8(self.layer8(x))\n",
    "        x = self.act9(self.layer9(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = Deep()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    " \n",
    "    n_epochs = 1000   # number of epochs to run\n",
    "    batch_size = 100  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        with tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            \n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "                \n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # print(f'Epoch {epoch}  loss {loss}   acc {acc}')\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_t = torch.tensor(y_train.values.astype(np.float32)).to(device)\n",
    "X_train_t = torch.tensor(X_train.values.astype(np.float32)).to(device)\n",
    "\n",
    "y_val_t = torch.tensor(y_val.values.astype(np.float32)).to(device)\n",
    "X_val_t = torch.tensor(X_val.values.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 79/79 [00:00<00:00, 173.62batch/s, acc=0.739, loss=0.569]\n",
      "Epoch 1: 100%|██████████| 79/79 [00:00<00:00, 185.89batch/s, acc=0.783, loss=0.467]\n",
      "Epoch 2: 100%|██████████| 79/79 [00:00<00:00, 191.67batch/s, acc=0.783, loss=0.445]\n",
      "Epoch 3: 100%|██████████| 79/79 [00:00<00:00, 175.65batch/s, acc=0.783, loss=0.418]\n",
      "Epoch 4: 100%|██████████| 79/79 [00:00<00:00, 182.79batch/s, acc=0.826, loss=0.393]\n",
      "Epoch 5: 100%|██████████| 79/79 [00:00<00:00, 188.26batch/s, acc=0.826, loss=0.366]\n",
      "Epoch 6: 100%|██████████| 79/79 [00:00<00:00, 175.47batch/s, acc=0.826, loss=0.349]\n",
      "Epoch 7: 100%|██████████| 79/79 [00:00<00:00, 186.35batch/s, acc=0.826, loss=0.338]\n",
      "Epoch 8: 100%|██████████| 79/79 [00:00<00:00, 187.42batch/s, acc=0.826, loss=0.333]\n",
      "Epoch 9: 100%|██████████| 79/79 [00:00<00:00, 184.95batch/s, acc=0.826, loss=0.33]\n",
      "Epoch 10: 100%|██████████| 79/79 [00:00<00:00, 180.02batch/s, acc=0.783, loss=0.334]\n",
      "Epoch 11: 100%|██████████| 79/79 [00:00<00:00, 180.67batch/s, acc=0.826, loss=0.321]\n",
      "Epoch 12: 100%|██████████| 79/79 [00:00<00:00, 171.93batch/s, acc=0.826, loss=0.31]\n",
      "Epoch 13: 100%|██████████| 79/79 [00:00<00:00, 169.13batch/s, acc=0.826, loss=0.304]\n",
      "Epoch 14: 100%|██████████| 79/79 [00:00<00:00, 158.22batch/s, acc=0.826, loss=0.298]\n",
      "Epoch 15: 100%|██████████| 79/79 [00:00<00:00, 166.61batch/s, acc=0.826, loss=0.294]\n",
      "Epoch 16: 100%|██████████| 79/79 [00:00<00:00, 161.63batch/s, acc=0.826, loss=0.288]\n",
      "Epoch 17: 100%|██████████| 79/79 [00:00<00:00, 169.90batch/s, acc=0.826, loss=0.283]\n",
      "Epoch 18: 100%|██████████| 79/79 [00:00<00:00, 176.31batch/s, acc=0.826, loss=0.285]\n",
      "Epoch 19: 100%|██████████| 79/79 [00:00<00:00, 161.93batch/s, acc=0.826, loss=0.287]\n",
      "Epoch 20: 100%|██████████| 79/79 [00:00<00:00, 160.06batch/s, acc=0.826, loss=0.284]\n",
      "Epoch 21: 100%|██████████| 79/79 [00:00<00:00, 165.56batch/s, acc=0.826, loss=0.28]\n",
      "Epoch 22: 100%|██████████| 79/79 [00:00<00:00, 166.10batch/s, acc=0.826, loss=0.277]\n",
      "Epoch 23: 100%|██████████| 79/79 [00:00<00:00, 164.18batch/s, acc=0.826, loss=0.28]\n",
      "Epoch 24: 100%|██████████| 79/79 [00:00<00:00, 167.10batch/s, acc=0.826, loss=0.279]\n",
      "Epoch 25: 100%|██████████| 79/79 [00:00<00:00, 165.24batch/s, acc=0.826, loss=0.278]\n",
      "Epoch 26: 100%|██████████| 79/79 [00:00<00:00, 172.51batch/s, acc=0.826, loss=0.281]\n",
      "Epoch 27: 100%|██████████| 79/79 [00:00<00:00, 168.51batch/s, acc=0.826, loss=0.281]\n",
      "Epoch 28: 100%|██████████| 79/79 [00:00<00:00, 145.93batch/s, acc=0.826, loss=0.282]\n",
      "Epoch 29: 100%|██████████| 79/79 [00:00<00:00, 157.92batch/s, acc=0.826, loss=0.283]\n",
      "Epoch 30: 100%|██████████| 79/79 [00:00<00:00, 159.34batch/s, acc=0.87, loss=0.269]\n",
      "Epoch 31: 100%|██████████| 79/79 [00:00<00:00, 166.37batch/s, acc=0.826, loss=0.277]\n",
      "Epoch 32: 100%|██████████| 79/79 [00:00<00:00, 155.90batch/s, acc=0.826, loss=0.28]\n",
      "Epoch 33: 100%|██████████| 79/79 [00:00<00:00, 165.89batch/s, acc=0.913, loss=0.281]\n",
      "Epoch 34: 100%|██████████| 79/79 [00:00<00:00, 151.84batch/s, acc=0.826, loss=0.276]\n",
      "Epoch 35: 100%|██████████| 79/79 [00:00<00:00, 163.37batch/s, acc=0.826, loss=0.273]\n",
      "Epoch 36: 100%|██████████| 79/79 [00:00<00:00, 153.56batch/s, acc=0.826, loss=0.287]\n",
      "Epoch 37: 100%|██████████| 79/79 [00:00<00:00, 164.02batch/s, acc=0.826, loss=0.295]\n",
      "Epoch 38: 100%|██████████| 79/79 [00:00<00:00, 155.84batch/s, acc=0.826, loss=0.288]\n",
      "Epoch 39: 100%|██████████| 79/79 [00:00<00:00, 163.25batch/s, acc=0.913, loss=0.288]\n",
      "Epoch 40: 100%|██████████| 79/79 [00:00<00:00, 158.39batch/s, acc=0.826, loss=0.288]\n",
      "Epoch 41: 100%|██████████| 79/79 [00:00<00:00, 175.70batch/s, acc=0.913, loss=0.266]\n",
      "Epoch 42: 100%|██████████| 79/79 [00:00<00:00, 154.19batch/s, acc=0.87, loss=0.296]\n",
      "Epoch 43: 100%|██████████| 79/79 [00:00<00:00, 163.04batch/s, acc=0.913, loss=0.277]\n",
      "Epoch 44: 100%|██████████| 79/79 [00:00<00:00, 163.94batch/s, acc=0.826, loss=0.292]\n",
      "Epoch 45: 100%|██████████| 79/79 [00:00<00:00, 163.14batch/s, acc=0.87, loss=0.303]\n",
      "Epoch 46: 100%|██████████| 79/79 [00:00<00:00, 157.77batch/s, acc=0.87, loss=0.3]  \n",
      "Epoch 47: 100%|██████████| 79/79 [00:00<00:00, 165.49batch/s, acc=0.913, loss=0.288]\n",
      "Epoch 48: 100%|██████████| 79/79 [00:00<00:00, 156.56batch/s, acc=0.826, loss=0.302]\n",
      "Epoch 49: 100%|██████████| 79/79 [00:00<00:00, 165.90batch/s, acc=0.826, loss=0.286]\n",
      "Epoch 50: 100%|██████████| 79/79 [00:00<00:00, 163.33batch/s, acc=0.913, loss=0.273]\n",
      "Epoch 51: 100%|██████████| 79/79 [00:00<00:00, 157.89batch/s, acc=0.87, loss=0.294]\n",
      "Epoch 52: 100%|██████████| 79/79 [00:00<00:00, 158.09batch/s, acc=0.913, loss=0.279]\n",
      "Epoch 53: 100%|██████████| 79/79 [00:00<00:00, 159.15batch/s, acc=0.913, loss=0.279]\n",
      "Epoch 54: 100%|██████████| 79/79 [00:00<00:00, 154.22batch/s, acc=0.913, loss=0.276]\n",
      "Epoch 55: 100%|██████████| 79/79 [00:00<00:00, 162.51batch/s, acc=0.913, loss=0.258]\n",
      "Epoch 56: 100%|██████████| 79/79 [00:00<00:00, 165.34batch/s, acc=0.913, loss=0.273]\n",
      "Epoch 57: 100%|██████████| 79/79 [00:00<00:00, 155.39batch/s, acc=0.826, loss=0.299]\n",
      "Epoch 58: 100%|██████████| 79/79 [00:00<00:00, 164.24batch/s, acc=0.826, loss=0.288]\n",
      "Epoch 59: 100%|██████████| 79/79 [00:00<00:00, 160.81batch/s, acc=0.913, loss=0.268]\n",
      "Epoch 60: 100%|██████████| 79/79 [00:00<00:00, 158.69batch/s, acc=0.87, loss=0.283]\n",
      "Epoch 61: 100%|██████████| 79/79 [00:00<00:00, 162.61batch/s, acc=0.913, loss=0.272]\n",
      "Epoch 62: 100%|██████████| 79/79 [00:00<00:00, 157.86batch/s, acc=0.913, loss=0.278]\n",
      "Epoch 63: 100%|██████████| 79/79 [00:00<00:00, 153.72batch/s, acc=0.913, loss=0.275]\n",
      "Epoch 64: 100%|██████████| 79/79 [00:00<00:00, 161.80batch/s, acc=0.913, loss=0.265]\n",
      "Epoch 65: 100%|██████████| 79/79 [00:00<00:00, 154.89batch/s, acc=0.913, loss=0.262]\n",
      "Epoch 66: 100%|██████████| 79/79 [00:00<00:00, 163.12batch/s, acc=0.87, loss=0.272]\n",
      "Epoch 67: 100%|██████████| 79/79 [00:00<00:00, 149.40batch/s, acc=0.913, loss=0.267]\n",
      "Epoch 68: 100%|██████████| 79/79 [00:00<00:00, 161.89batch/s, acc=0.826, loss=0.276]\n",
      "Epoch 69: 100%|██████████| 79/79 [00:00<00:00, 152.44batch/s, acc=0.87, loss=0.262]\n",
      "Epoch 70: 100%|██████████| 79/79 [00:00<00:00, 162.20batch/s, acc=0.913, loss=0.262]\n",
      "Epoch 71: 100%|██████████| 79/79 [00:00<00:00, 148.52batch/s, acc=0.913, loss=0.259]\n",
      "Epoch 72: 100%|██████████| 79/79 [00:00<00:00, 160.97batch/s, acc=0.913, loss=0.255]\n",
      "Epoch 73: 100%|██████████| 79/79 [00:00<00:00, 165.20batch/s, acc=0.826, loss=0.288]\n",
      "Epoch 74: 100%|██████████| 79/79 [00:00<00:00, 162.02batch/s, acc=0.913, loss=0.267]\n",
      "Epoch 75: 100%|██████████| 79/79 [00:00<00:00, 157.26batch/s, acc=0.826, loss=0.289]\n",
      "Epoch 76: 100%|██████████| 79/79 [00:00<00:00, 158.22batch/s, acc=0.913, loss=0.253]\n",
      "Epoch 77: 100%|██████████| 79/79 [00:00<00:00, 159.32batch/s, acc=0.913, loss=0.276]\n",
      "Epoch 78: 100%|██████████| 79/79 [00:00<00:00, 162.12batch/s, acc=0.87, loss=0.284]\n",
      "Epoch 79: 100%|██████████| 79/79 [00:00<00:00, 158.42batch/s, acc=0.913, loss=0.242]\n",
      "Epoch 80: 100%|██████████| 79/79 [00:00<00:00, 161.83batch/s, acc=0.913, loss=0.242]\n",
      "Epoch 81: 100%|██████████| 79/79 [00:00<00:00, 159.61batch/s, acc=0.913, loss=0.244]\n",
      "Epoch 82: 100%|██████████| 79/79 [00:00<00:00, 159.84batch/s, acc=0.913, loss=0.218]\n",
      "Epoch 83: 100%|██████████| 79/79 [00:00<00:00, 167.75batch/s, acc=0.87, loss=0.243]\n",
      "Epoch 84: 100%|██████████| 79/79 [00:00<00:00, 162.98batch/s, acc=0.913, loss=0.222]\n",
      "Epoch 85: 100%|██████████| 79/79 [00:00<00:00, 150.91batch/s, acc=0.913, loss=0.224]\n",
      "Epoch 86: 100%|██████████| 79/79 [00:00<00:00, 153.43batch/s, acc=0.913, loss=0.214]\n",
      "Epoch 87: 100%|██████████| 79/79 [00:00<00:00, 161.67batch/s, acc=0.913, loss=0.244]\n",
      "Epoch 88: 100%|██████████| 79/79 [00:00<00:00, 149.32batch/s, acc=0.87, loss=0.262]\n",
      "Epoch 89: 100%|██████████| 79/79 [00:00<00:00, 164.37batch/s, acc=0.913, loss=0.221]\n",
      "Epoch 90: 100%|██████████| 79/79 [00:00<00:00, 158.15batch/s, acc=0.913, loss=0.242]\n",
      "Epoch 91: 100%|██████████| 79/79 [00:00<00:00, 167.25batch/s, acc=0.913, loss=0.228]\n",
      "Epoch 92: 100%|██████████| 79/79 [00:00<00:00, 160.32batch/s, acc=0.913, loss=0.24]\n",
      "Epoch 93: 100%|██████████| 79/79 [00:00<00:00, 164.08batch/s, acc=0.87, loss=0.243]\n",
      "Epoch 94: 100%|██████████| 79/79 [00:00<00:00, 152.56batch/s, acc=0.913, loss=0.222]\n",
      "Epoch 95: 100%|██████████| 79/79 [00:00<00:00, 161.23batch/s, acc=0.913, loss=0.216]\n",
      "Epoch 96: 100%|██████████| 79/79 [00:00<00:00, 158.02batch/s, acc=0.913, loss=0.2] \n",
      "Epoch 97: 100%|██████████| 79/79 [00:00<00:00, 156.68batch/s, acc=0.913, loss=0.211]\n",
      "Epoch 98: 100%|██████████| 79/79 [00:00<00:00, 159.84batch/s, acc=0.913, loss=0.219]\n",
      "Epoch 99: 100%|██████████| 79/79 [00:00<00:00, 169.33batch/s, acc=0.826, loss=0.223]\n",
      "Epoch 100: 100%|██████████| 79/79 [00:00<00:00, 157.50batch/s, acc=0.913, loss=0.222]\n",
      "Epoch 101: 100%|██████████| 79/79 [00:00<00:00, 152.98batch/s, acc=0.913, loss=0.198]\n",
      "Epoch 102: 100%|██████████| 79/79 [00:00<00:00, 148.22batch/s, acc=0.913, loss=0.193]\n",
      "Epoch 103: 100%|██████████| 79/79 [00:00<00:00, 160.26batch/s, acc=0.913, loss=0.204]\n",
      "Epoch 104: 100%|██████████| 79/79 [00:00<00:00, 148.23batch/s, acc=0.913, loss=0.193]\n",
      "Epoch 105: 100%|██████████| 79/79 [00:00<00:00, 164.33batch/s, acc=0.913, loss=0.185]\n",
      "Epoch 106: 100%|██████████| 79/79 [00:00<00:00, 155.08batch/s, acc=0.913, loss=0.181]\n",
      "Epoch 107: 100%|██████████| 79/79 [00:00<00:00, 158.73batch/s, acc=0.913, loss=0.223]\n",
      "Epoch 108: 100%|██████████| 79/79 [00:00<00:00, 150.42batch/s, acc=0.913, loss=0.187]\n",
      "Epoch 109: 100%|██████████| 79/79 [00:00<00:00, 157.59batch/s, acc=0.913, loss=0.196]\n",
      "Epoch 110: 100%|██████████| 79/79 [00:00<00:00, 150.11batch/s, acc=0.913, loss=0.187]\n",
      "Epoch 111: 100%|██████████| 79/79 [00:00<00:00, 164.50batch/s, acc=0.913, loss=0.186]\n",
      "Epoch 112: 100%|██████████| 79/79 [00:00<00:00, 153.67batch/s, acc=0.913, loss=0.191]\n",
      "Epoch 113: 100%|██████████| 79/79 [00:00<00:00, 157.13batch/s, acc=0.913, loss=0.205]\n",
      "Epoch 114: 100%|██████████| 79/79 [00:00<00:00, 151.37batch/s, acc=0.913, loss=0.195]\n",
      "Epoch 115: 100%|██████████| 79/79 [00:00<00:00, 161.22batch/s, acc=0.913, loss=0.211]\n",
      "Epoch 116: 100%|██████████| 79/79 [00:00<00:00, 146.64batch/s, acc=0.913, loss=0.193]\n",
      "Epoch 117: 100%|██████████| 79/79 [00:00<00:00, 161.36batch/s, acc=0.913, loss=0.197]\n",
      "Epoch 118: 100%|██████████| 79/79 [00:00<00:00, 152.89batch/s, acc=0.913, loss=0.179]\n",
      "Epoch 119: 100%|██████████| 79/79 [00:00<00:00, 152.57batch/s, acc=0.913, loss=0.176]\n",
      "Epoch 120: 100%|██████████| 79/79 [00:00<00:00, 158.26batch/s, acc=0.913, loss=0.181]\n",
      "Epoch 121: 100%|██████████| 79/79 [00:00<00:00, 161.45batch/s, acc=0.913, loss=0.205]\n",
      "Epoch 122: 100%|██████████| 79/79 [00:00<00:00, 157.98batch/s, acc=0.913, loss=0.21]\n",
      "Epoch 123: 100%|██████████| 79/79 [00:00<00:00, 151.18batch/s, acc=0.913, loss=0.19]\n",
      "Epoch 124: 100%|██████████| 79/79 [00:00<00:00, 159.68batch/s, acc=0.913, loss=0.198]\n",
      "Epoch 125: 100%|██████████| 79/79 [00:00<00:00, 157.56batch/s, acc=0.913, loss=0.198]\n",
      "Epoch 126: 100%|██████████| 79/79 [00:00<00:00, 159.72batch/s, acc=0.913, loss=0.181]\n",
      "Epoch 127: 100%|██████████| 79/79 [00:00<00:00, 150.74batch/s, acc=0.913, loss=0.2] \n",
      "Epoch 128: 100%|██████████| 79/79 [00:00<00:00, 154.71batch/s, acc=0.913, loss=0.165]\n",
      "Epoch 129: 100%|██████████| 79/79 [00:00<00:00, 160.48batch/s, acc=0.913, loss=0.189]\n",
      "Epoch 130: 100%|██████████| 79/79 [00:00<00:00, 149.25batch/s, acc=0.913, loss=0.174]\n",
      "Epoch 131: 100%|██████████| 79/79 [00:00<00:00, 151.60batch/s, acc=0.913, loss=0.177]\n",
      "Epoch 132: 100%|██████████| 79/79 [00:00<00:00, 156.50batch/s, acc=0.913, loss=0.169]\n",
      "Epoch 133: 100%|██████████| 79/79 [00:00<00:00, 147.96batch/s, acc=0.913, loss=0.185]\n",
      "Epoch 134: 100%|██████████| 79/79 [00:00<00:00, 152.24batch/s, acc=0.913, loss=0.164]\n",
      "Epoch 135: 100%|██████████| 79/79 [00:00<00:00, 151.12batch/s, acc=0.913, loss=0.169]\n",
      "Epoch 136: 100%|██████████| 79/79 [00:00<00:00, 147.65batch/s, acc=0.913, loss=0.16]\n",
      "Epoch 137: 100%|██████████| 79/79 [00:00<00:00, 152.72batch/s, acc=0.913, loss=0.166]\n",
      "Epoch 138: 100%|██████████| 79/79 [00:00<00:00, 152.64batch/s, acc=0.913, loss=0.175]\n",
      "Epoch 139: 100%|██████████| 79/79 [00:00<00:00, 147.99batch/s, acc=0.913, loss=0.166]\n",
      "Epoch 140: 100%|██████████| 79/79 [00:00<00:00, 155.68batch/s, acc=0.913, loss=0.184]\n",
      "Epoch 141: 100%|██████████| 79/79 [00:00<00:00, 156.16batch/s, acc=0.87, loss=0.188]\n",
      "Epoch 142: 100%|██████████| 79/79 [00:00<00:00, 164.37batch/s, acc=0.913, loss=0.173]\n",
      "Epoch 143: 100%|██████████| 79/79 [00:00<00:00, 167.92batch/s, acc=0.913, loss=0.165]\n",
      "Epoch 144: 100%|██████████| 79/79 [00:00<00:00, 165.32batch/s, acc=0.913, loss=0.166]\n",
      "Epoch 145: 100%|██████████| 79/79 [00:00<00:00, 182.25batch/s, acc=0.913, loss=0.164]\n",
      "Epoch 146: 100%|██████████| 79/79 [00:00<00:00, 189.67batch/s, acc=0.913, loss=0.16]\n",
      "Epoch 147: 100%|██████████| 79/79 [00:00<00:00, 168.87batch/s, acc=0.913, loss=0.165]\n",
      "Epoch 148: 100%|██████████| 79/79 [00:00<00:00, 175.57batch/s, acc=0.913, loss=0.155]\n",
      "Epoch 149: 100%|██████████| 79/79 [00:00<00:00, 175.85batch/s, acc=0.957, loss=0.164]\n",
      "Epoch 150: 100%|██████████| 79/79 [00:00<00:00, 166.99batch/s, acc=0.913, loss=0.155]\n",
      "Epoch 151: 100%|██████████| 79/79 [00:00<00:00, 164.65batch/s, acc=0.913, loss=0.147]\n",
      "Epoch 152: 100%|██████████| 79/79 [00:00<00:00, 170.68batch/s, acc=0.957, loss=0.139]\n",
      "Epoch 153: 100%|██████████| 79/79 [00:00<00:00, 165.24batch/s, acc=0.913, loss=0.141]\n",
      "Epoch 154: 100%|██████████| 79/79 [00:00<00:00, 170.53batch/s, acc=0.913, loss=0.16]\n",
      "Epoch 155: 100%|██████████| 79/79 [00:00<00:00, 169.40batch/s, acc=0.957, loss=0.135]\n",
      "Epoch 156: 100%|██████████| 79/79 [00:00<00:00, 172.56batch/s, acc=0.957, loss=0.13]\n",
      "Epoch 157: 100%|██████████| 79/79 [00:00<00:00, 178.24batch/s, acc=0.913, loss=0.132]\n",
      "Epoch 158: 100%|██████████| 79/79 [00:00<00:00, 168.73batch/s, acc=0.913, loss=0.142]\n",
      "Epoch 159: 100%|██████████| 79/79 [00:00<00:00, 178.88batch/s, acc=0.957, loss=0.14]\n",
      "Epoch 160: 100%|██████████| 79/79 [00:00<00:00, 161.08batch/s, acc=0.913, loss=0.157]\n",
      "Epoch 161: 100%|██████████| 79/79 [00:00<00:00, 180.08batch/s, acc=0.913, loss=0.138]\n",
      "Epoch 162: 100%|██████████| 79/79 [00:00<00:00, 173.91batch/s, acc=0.957, loss=0.16]\n",
      "Epoch 163: 100%|██████████| 79/79 [00:00<00:00, 161.80batch/s, acc=0.957, loss=0.144]\n",
      "Epoch 164: 100%|██████████| 79/79 [00:00<00:00, 173.09batch/s, acc=0.957, loss=0.134]\n",
      "Epoch 165: 100%|██████████| 79/79 [00:00<00:00, 154.43batch/s, acc=0.957, loss=0.124]\n",
      "Epoch 166: 100%|██████████| 79/79 [00:00<00:00, 146.80batch/s, acc=0.957, loss=0.119]\n",
      "Epoch 167: 100%|██████████| 79/79 [00:00<00:00, 153.66batch/s, acc=0.957, loss=0.124]\n",
      "Epoch 168: 100%|██████████| 79/79 [00:00<00:00, 155.34batch/s, acc=0.913, loss=0.132]\n",
      "Epoch 169: 100%|██████████| 79/79 [00:00<00:00, 156.52batch/s, acc=0.957, loss=0.113]\n",
      "Epoch 170: 100%|██████████| 79/79 [00:00<00:00, 158.92batch/s, acc=0.957, loss=0.117]\n",
      "Epoch 171: 100%|██████████| 79/79 [00:00<00:00, 154.49batch/s, acc=0.913, loss=0.136]\n",
      "Epoch 172: 100%|██████████| 79/79 [00:00<00:00, 156.04batch/s, acc=0.913, loss=0.141]\n",
      "Epoch 173: 100%|██████████| 79/79 [00:00<00:00, 155.74batch/s, acc=0.913, loss=0.165]\n",
      "Epoch 174: 100%|██████████| 79/79 [00:00<00:00, 153.89batch/s, acc=0.913, loss=0.124]\n",
      "Epoch 175: 100%|██████████| 79/79 [00:00<00:00, 156.16batch/s, acc=0.957, loss=0.15]\n",
      "Epoch 176: 100%|██████████| 79/79 [00:00<00:00, 158.43batch/s, acc=0.913, loss=0.116]\n",
      "Epoch 177: 100%|██████████| 79/79 [00:00<00:00, 160.61batch/s, acc=0.957, loss=0.14]\n",
      "Epoch 178: 100%|██████████| 79/79 [00:00<00:00, 158.07batch/s, acc=0.913, loss=0.16]\n",
      "Epoch 179: 100%|██████████| 79/79 [00:00<00:00, 158.48batch/s, acc=0.913, loss=0.174]\n",
      "Epoch 180: 100%|██████████| 79/79 [00:00<00:00, 157.06batch/s, acc=0.957, loss=0.136]\n",
      "Epoch 181: 100%|██████████| 79/79 [00:00<00:00, 157.46batch/s, acc=0.957, loss=0.123]\n",
      "Epoch 182: 100%|██████████| 79/79 [00:00<00:00, 154.64batch/s, acc=0.913, loss=0.12]\n",
      "Epoch 183: 100%|██████████| 79/79 [00:00<00:00, 154.67batch/s, acc=0.957, loss=0.162]\n",
      "Epoch 184: 100%|██████████| 79/79 [00:00<00:00, 156.10batch/s, acc=0.957, loss=0.107]\n",
      "Epoch 185: 100%|██████████| 79/79 [00:00<00:00, 160.85batch/s, acc=0.913, loss=0.117]\n",
      "Epoch 186: 100%|██████████| 79/79 [00:00<00:00, 153.86batch/s, acc=0.957, loss=0.111]\n",
      "Epoch 187: 100%|██████████| 79/79 [00:00<00:00, 155.60batch/s, acc=0.957, loss=0.119]\n",
      "Epoch 188: 100%|██████████| 79/79 [00:00<00:00, 156.40batch/s, acc=0.957, loss=0.137]\n",
      "Epoch 189: 100%|██████████| 79/79 [00:00<00:00, 145.67batch/s, acc=0.913, loss=0.115]\n",
      "Epoch 190: 100%|██████████| 79/79 [00:00<00:00, 156.87batch/s, acc=0.957, loss=0.121]\n",
      "Epoch 191: 100%|██████████| 79/79 [00:00<00:00, 155.51batch/s, acc=0.957, loss=0.133]\n",
      "Epoch 192: 100%|██████████| 79/79 [00:00<00:00, 151.92batch/s, acc=0.957, loss=0.12]\n",
      "Epoch 193: 100%|██████████| 79/79 [00:00<00:00, 157.86batch/s, acc=0.957, loss=0.137]\n",
      "Epoch 194: 100%|██████████| 79/79 [00:00<00:00, 146.44batch/s, acc=0.957, loss=0.134]\n",
      "Epoch 195: 100%|██████████| 79/79 [00:00<00:00, 152.59batch/s, acc=0.957, loss=0.114]\n",
      "Epoch 196: 100%|██████████| 79/79 [00:00<00:00, 157.62batch/s, acc=0.913, loss=0.106]\n",
      "Epoch 197: 100%|██████████| 79/79 [00:00<00:00, 152.68batch/s, acc=0.913, loss=0.132]\n",
      "Epoch 198: 100%|██████████| 79/79 [00:00<00:00, 138.72batch/s, acc=0.957, loss=0.124]\n",
      "Epoch 199: 100%|██████████| 79/79 [00:00<00:00, 157.03batch/s, acc=0.957, loss=0.121]\n",
      "Epoch 200: 100%|██████████| 79/79 [00:00<00:00, 154.33batch/s, acc=0.957, loss=0.122]\n",
      "Epoch 201: 100%|██████████| 79/79 [00:00<00:00, 152.54batch/s, acc=0.957, loss=0.135]\n",
      "Epoch 202: 100%|██████████| 79/79 [00:00<00:00, 164.06batch/s, acc=0.957, loss=0.108]\n",
      "Epoch 203: 100%|██████████| 79/79 [00:00<00:00, 156.74batch/s, acc=0.957, loss=0.113]\n",
      "Epoch 204: 100%|██████████| 79/79 [00:00<00:00, 152.63batch/s, acc=0.957, loss=0.138]\n",
      "Epoch 205: 100%|██████████| 79/79 [00:00<00:00, 149.32batch/s, acc=0.913, loss=0.131]\n",
      "Epoch 206: 100%|██████████| 79/79 [00:00<00:00, 157.56batch/s, acc=0.913, loss=0.15]\n",
      "Epoch 207: 100%|██████████| 79/79 [00:00<00:00, 168.59batch/s, acc=0.957, loss=0.114]\n",
      "Epoch 208: 100%|██████████| 79/79 [00:00<00:00, 156.67batch/s, acc=0.957, loss=0.119]\n",
      "Epoch 209: 100%|██████████| 79/79 [00:00<00:00, 145.06batch/s, acc=0.957, loss=0.117]\n",
      "Epoch 210: 100%|██████████| 79/79 [00:00<00:00, 151.99batch/s, acc=0.957, loss=0.122]\n",
      "Epoch 211: 100%|██████████| 79/79 [00:00<00:00, 151.29batch/s, acc=0.957, loss=0.119]\n",
      "Epoch 212: 100%|██████████| 79/79 [00:00<00:00, 148.71batch/s, acc=0.957, loss=0.139]\n",
      "Epoch 213: 100%|██████████| 79/79 [00:00<00:00, 158.31batch/s, acc=0.957, loss=0.141]\n",
      "Epoch 214: 100%|██████████| 79/79 [00:00<00:00, 154.00batch/s, acc=0.957, loss=0.104]\n",
      "Epoch 215: 100%|██████████| 79/79 [00:00<00:00, 156.58batch/s, acc=0.957, loss=0.142]\n",
      "Epoch 216: 100%|██████████| 79/79 [00:00<00:00, 147.76batch/s, acc=0.957, loss=0.112]\n",
      "Epoch 217: 100%|██████████| 79/79 [00:00<00:00, 166.70batch/s, acc=0.957, loss=0.119]\n",
      "Epoch 218: 100%|██████████| 79/79 [00:00<00:00, 160.94batch/s, acc=0.957, loss=0.128]\n",
      "Epoch 219: 100%|██████████| 79/79 [00:00<00:00, 155.82batch/s, acc=0.957, loss=0.119]\n",
      "Epoch 220: 100%|██████████| 79/79 [00:00<00:00, 156.89batch/s, acc=0.957, loss=0.131]\n",
      "Epoch 221: 100%|██████████| 79/79 [00:00<00:00, 154.11batch/s, acc=0.957, loss=0.133]\n",
      "Epoch 222: 100%|██████████| 79/79 [00:00<00:00, 156.16batch/s, acc=0.957, loss=0.138]\n",
      "Epoch 223: 100%|██████████| 79/79 [00:00<00:00, 159.01batch/s, acc=0.913, loss=0.134]\n",
      "Epoch 224: 100%|██████████| 79/79 [00:00<00:00, 159.29batch/s, acc=0.913, loss=0.117]\n",
      "Epoch 225: 100%|██████████| 79/79 [00:00<00:00, 154.39batch/s, acc=0.957, loss=0.11]\n",
      "Epoch 226: 100%|██████████| 79/79 [00:00<00:00, 158.98batch/s, acc=0.957, loss=0.0942]\n",
      "Epoch 227: 100%|██████████| 79/79 [00:00<00:00, 154.25batch/s, acc=0.957, loss=0.0923]\n",
      "Epoch 228: 100%|██████████| 79/79 [00:00<00:00, 153.19batch/s, acc=0.957, loss=0.0969]\n",
      "Epoch 229: 100%|██████████| 79/79 [00:00<00:00, 153.08batch/s, acc=0.957, loss=0.0978]\n",
      "Epoch 230: 100%|██████████| 79/79 [00:00<00:00, 151.61batch/s, acc=0.913, loss=0.13]\n",
      "Epoch 231: 100%|██████████| 79/79 [00:00<00:00, 156.93batch/s, acc=0.957, loss=0.102]\n",
      "Epoch 232: 100%|██████████| 79/79 [00:00<00:00, 158.35batch/s, acc=0.957, loss=0.087]\n",
      "Epoch 233: 100%|██████████| 79/79 [00:00<00:00, 159.97batch/s, acc=0.957, loss=0.0947]\n",
      "Epoch 234: 100%|██████████| 79/79 [00:00<00:00, 149.93batch/s, acc=0.957, loss=0.116]\n",
      "Epoch 235: 100%|██████████| 79/79 [00:00<00:00, 152.95batch/s, acc=0.957, loss=0.102]\n",
      "Epoch 236: 100%|██████████| 79/79 [00:00<00:00, 154.11batch/s, acc=0.957, loss=0.101]\n",
      "Epoch 237: 100%|██████████| 79/79 [00:00<00:00, 155.76batch/s, acc=0.957, loss=0.125]\n",
      "Epoch 238: 100%|██████████| 79/79 [00:00<00:00, 154.55batch/s, acc=0.957, loss=0.0972]\n",
      "Epoch 239: 100%|██████████| 79/79 [00:00<00:00, 154.32batch/s, acc=0.957, loss=0.0979]\n",
      "Epoch 240: 100%|██████████| 79/79 [00:00<00:00, 154.54batch/s, acc=0.957, loss=0.103]\n",
      "Epoch 241: 100%|██████████| 79/79 [00:00<00:00, 155.08batch/s, acc=0.957, loss=0.1] \n",
      "Epoch 242: 100%|██████████| 79/79 [00:00<00:00, 160.38batch/s, acc=0.957, loss=0.186]\n",
      "Epoch 243: 100%|██████████| 79/79 [00:00<00:00, 146.85batch/s, acc=0.957, loss=0.121]\n",
      "Epoch 244: 100%|██████████| 79/79 [00:00<00:00, 165.62batch/s, acc=0.957, loss=0.109]\n",
      "Epoch 245: 100%|██████████| 79/79 [00:00<00:00, 152.51batch/s, acc=0.957, loss=0.0956]\n",
      "Epoch 246: 100%|██████████| 79/79 [00:00<00:00, 153.97batch/s, acc=0.957, loss=0.0802]\n",
      "Epoch 247: 100%|██████████| 79/79 [00:00<00:00, 158.08batch/s, acc=0.957, loss=0.0858]\n",
      "Epoch 248: 100%|██████████| 79/79 [00:00<00:00, 151.36batch/s, acc=0.957, loss=0.11]\n",
      "Epoch 249: 100%|██████████| 79/79 [00:00<00:00, 153.28batch/s, acc=0.957, loss=0.0883]\n",
      "Epoch 250: 100%|██████████| 79/79 [00:00<00:00, 156.84batch/s, acc=0.957, loss=0.0939]\n",
      "Epoch 251: 100%|██████████| 79/79 [00:00<00:00, 156.65batch/s, acc=0.957, loss=0.0915]\n",
      "Epoch 252: 100%|██████████| 79/79 [00:00<00:00, 161.36batch/s, acc=0.913, loss=0.113]\n",
      "Epoch 253: 100%|██████████| 79/79 [00:00<00:00, 150.10batch/s, acc=0.957, loss=0.0916]\n",
      "Epoch 254: 100%|██████████| 79/79 [00:00<00:00, 163.83batch/s, acc=0.957, loss=0.0971]\n",
      "Epoch 255: 100%|██████████| 79/79 [00:00<00:00, 154.53batch/s, acc=0.957, loss=0.105]\n",
      "Epoch 256: 100%|██████████| 79/79 [00:00<00:00, 152.87batch/s, acc=0.913, loss=0.104]\n",
      "Epoch 257: 100%|██████████| 79/79 [00:00<00:00, 158.35batch/s, acc=0.913, loss=0.114]\n",
      "Epoch 258: 100%|██████████| 79/79 [00:00<00:00, 155.38batch/s, acc=0.957, loss=0.0963]\n",
      "Epoch 259: 100%|██████████| 79/79 [00:00<00:00, 155.01batch/s, acc=0.913, loss=0.146]\n",
      "Epoch 260: 100%|██████████| 79/79 [00:00<00:00, 154.84batch/s, acc=0.957, loss=0.0853]\n",
      "Epoch 261: 100%|██████████| 79/79 [00:00<00:00, 146.17batch/s, acc=0.957, loss=0.0908]\n",
      "Epoch 262: 100%|██████████| 79/79 [00:00<00:00, 153.25batch/s, acc=0.957, loss=0.102]\n",
      "Epoch 263: 100%|██████████| 79/79 [00:00<00:00, 149.51batch/s, acc=0.957, loss=0.0937]\n",
      "Epoch 264: 100%|██████████| 79/79 [00:00<00:00, 145.83batch/s, acc=0.957, loss=0.0677]\n",
      "Epoch 265: 100%|██████████| 79/79 [00:00<00:00, 152.30batch/s, acc=0.957, loss=0.0991]\n",
      "Epoch 266: 100%|██████████| 79/79 [00:00<00:00, 161.42batch/s, acc=0.957, loss=0.0857]\n",
      "Epoch 267: 100%|██████████| 79/79 [00:00<00:00, 145.40batch/s, acc=0.957, loss=0.0747]\n",
      "Epoch 268: 100%|██████████| 79/79 [00:00<00:00, 150.82batch/s, acc=0.957, loss=0.0786]\n",
      "Epoch 269: 100%|██████████| 79/79 [00:00<00:00, 152.56batch/s, acc=0.957, loss=0.0838]\n",
      "Epoch 270: 100%|██████████| 79/79 [00:00<00:00, 150.85batch/s, acc=0.957, loss=0.111]\n",
      "Epoch 271: 100%|██████████| 79/79 [00:00<00:00, 157.88batch/s, acc=0.957, loss=0.0968]\n",
      "Epoch 272: 100%|██████████| 79/79 [00:00<00:00, 158.23batch/s, acc=0.957, loss=0.0846]\n",
      "Epoch 273: 100%|██████████| 79/79 [00:00<00:00, 154.60batch/s, acc=0.957, loss=0.0742]\n",
      "Epoch 274: 100%|██████████| 79/79 [00:00<00:00, 144.54batch/s, acc=0.957, loss=0.0831]\n",
      "Epoch 275: 100%|██████████| 79/79 [00:00<00:00, 150.69batch/s, acc=0.957, loss=0.0831]\n",
      "Epoch 276: 100%|██████████| 79/79 [00:00<00:00, 153.77batch/s, acc=0.957, loss=0.0688]\n",
      "Epoch 277: 100%|██████████| 79/79 [00:00<00:00, 144.45batch/s, acc=0.957, loss=0.116]\n",
      "Epoch 278: 100%|██████████| 79/79 [00:00<00:00, 145.36batch/s, acc=0.957, loss=0.0988]\n",
      "Epoch 279: 100%|██████████| 79/79 [00:00<00:00, 153.70batch/s, acc=0.957, loss=0.0744]\n",
      "Epoch 280: 100%|██████████| 79/79 [00:00<00:00, 147.06batch/s, acc=0.957, loss=0.093]\n",
      "Epoch 281: 100%|██████████| 79/79 [00:00<00:00, 140.83batch/s, acc=0.957, loss=0.102]\n",
      "Epoch 282: 100%|██████████| 79/79 [00:00<00:00, 140.51batch/s, acc=0.957, loss=0.0792]\n",
      "Epoch 283: 100%|██████████| 79/79 [00:00<00:00, 139.73batch/s, acc=0.957, loss=0.0723]\n",
      "Epoch 284: 100%|██████████| 79/79 [00:00<00:00, 142.57batch/s, acc=0.957, loss=0.0669]\n",
      "Epoch 285: 100%|██████████| 79/79 [00:00<00:00, 151.06batch/s, acc=0.957, loss=0.0757]\n",
      "Epoch 286: 100%|██████████| 79/79 [00:00<00:00, 127.64batch/s, acc=0.957, loss=0.0643]\n",
      "Epoch 287: 100%|██████████| 79/79 [00:00<00:00, 117.88batch/s, acc=0.957, loss=0.0618]\n",
      "Epoch 288: 100%|██████████| 79/79 [00:00<00:00, 131.81batch/s, acc=1, loss=0.096]    \n",
      "Epoch 289: 100%|██████████| 79/79 [00:00<00:00, 131.33batch/s, acc=0.957, loss=0.0739]\n",
      "Epoch 290: 100%|██████████| 79/79 [00:00<00:00, 124.61batch/s, acc=0.957, loss=0.0915]\n",
      "Epoch 291: 100%|██████████| 79/79 [00:00<00:00, 126.17batch/s, acc=0.957, loss=0.0899]\n",
      "Epoch 292: 100%|██████████| 79/79 [00:00<00:00, 139.05batch/s, acc=0.957, loss=0.0961]\n",
      "Epoch 293: 100%|██████████| 79/79 [00:00<00:00, 148.08batch/s, acc=0.957, loss=0.102]\n",
      "Epoch 294: 100%|██████████| 79/79 [00:00<00:00, 154.39batch/s, acc=0.957, loss=0.0966]\n",
      "Epoch 295: 100%|██████████| 79/79 [00:00<00:00, 147.94batch/s, acc=0.957, loss=0.11]\n",
      "Epoch 296: 100%|██████████| 79/79 [00:00<00:00, 146.84batch/s, acc=0.957, loss=0.098]\n",
      "Epoch 297: 100%|██████████| 79/79 [00:00<00:00, 139.50batch/s, acc=0.957, loss=0.118]\n",
      "Epoch 298: 100%|██████████| 79/79 [00:00<00:00, 146.72batch/s, acc=0.957, loss=0.0688]\n",
      "Epoch 299: 100%|██████████| 79/79 [00:00<00:00, 144.21batch/s, acc=0.957, loss=0.092]\n",
      "Epoch 300: 100%|██████████| 79/79 [00:00<00:00, 145.15batch/s, acc=0.957, loss=0.115]\n",
      "Epoch 301: 100%|██████████| 79/79 [00:00<00:00, 161.11batch/s, acc=0.957, loss=0.0826]\n",
      "Epoch 302: 100%|██████████| 79/79 [00:00<00:00, 171.48batch/s, acc=0.957, loss=0.0831]\n",
      "Epoch 303: 100%|██████████| 79/79 [00:00<00:00, 191.46batch/s, acc=0.957, loss=0.068]\n",
      "Epoch 304: 100%|██████████| 79/79 [00:00<00:00, 160.39batch/s, acc=0.957, loss=0.0798]\n",
      "Epoch 305: 100%|██████████| 79/79 [00:00<00:00, 162.42batch/s, acc=0.957, loss=0.128]\n",
      "Epoch 306: 100%|██████████| 79/79 [00:00<00:00, 172.71batch/s, acc=0.957, loss=0.111]\n",
      "Epoch 307: 100%|██████████| 79/79 [00:00<00:00, 157.62batch/s, acc=1, loss=0.076]   \n",
      "Epoch 308: 100%|██████████| 79/79 [00:00<00:00, 153.47batch/s, acc=1, loss=0.126]   \n",
      "Epoch 309: 100%|██████████| 79/79 [00:00<00:00, 170.62batch/s, acc=0.957, loss=0.0837]\n",
      "Epoch 310: 100%|██████████| 79/79 [00:00<00:00, 153.95batch/s, acc=0.957, loss=0.0775]\n",
      "Epoch 311: 100%|██████████| 79/79 [00:00<00:00, 158.20batch/s, acc=0.957, loss=0.101]\n",
      "Epoch 312: 100%|██████████| 79/79 [00:00<00:00, 150.92batch/s, acc=0.957, loss=0.115]\n",
      "Epoch 313: 100%|██████████| 79/79 [00:00<00:00, 151.36batch/s, acc=0.957, loss=0.0991]\n",
      "Epoch 314: 100%|██████████| 79/79 [00:00<00:00, 154.59batch/s, acc=0.957, loss=0.115]\n",
      "Epoch 315: 100%|██████████| 79/79 [00:00<00:00, 158.78batch/s, acc=0.957, loss=0.0721]\n",
      "Epoch 316: 100%|██████████| 79/79 [00:00<00:00, 144.73batch/s, acc=1, loss=0.0775]  \n",
      "Epoch 317: 100%|██████████| 79/79 [00:00<00:00, 146.15batch/s, acc=1, loss=0.069]    \n",
      "Epoch 318: 100%|██████████| 79/79 [00:00<00:00, 157.11batch/s, acc=0.957, loss=0.078]\n",
      "Epoch 319: 100%|██████████| 79/79 [00:00<00:00, 159.69batch/s, acc=1, loss=0.106]   \n",
      "Epoch 320: 100%|██████████| 79/79 [00:00<00:00, 155.31batch/s, acc=0.957, loss=0.08] \n",
      "Epoch 321: 100%|██████████| 79/79 [00:00<00:00, 152.64batch/s, acc=0.957, loss=0.0555]\n",
      "Epoch 322: 100%|██████████| 79/79 [00:00<00:00, 154.70batch/s, acc=0.957, loss=0.0791]\n",
      "Epoch 323: 100%|██████████| 79/79 [00:00<00:00, 156.55batch/s, acc=0.957, loss=0.0596]\n",
      "Epoch 324: 100%|██████████| 79/79 [00:00<00:00, 154.55batch/s, acc=0.957, loss=0.0691]\n",
      "Epoch 325: 100%|██████████| 79/79 [00:00<00:00, 154.32batch/s, acc=1, loss=0.0575]   \n",
      "Epoch 326: 100%|██████████| 79/79 [00:00<00:00, 154.08batch/s, acc=1, loss=0.066]    \n",
      "Epoch 327: 100%|██████████| 79/79 [00:00<00:00, 152.10batch/s, acc=0.957, loss=0.0748]\n",
      "Epoch 328: 100%|██████████| 79/79 [00:00<00:00, 136.76batch/s, acc=1, loss=0.0607]   \n",
      "Epoch 329: 100%|██████████| 79/79 [00:00<00:00, 138.26batch/s, acc=1, loss=0.0691]   \n",
      "Epoch 330: 100%|██████████| 79/79 [00:00<00:00, 137.50batch/s, acc=0.957, loss=0.0716]\n",
      "Epoch 331: 100%|██████████| 79/79 [00:00<00:00, 144.21batch/s, acc=1, loss=0.0492]   \n",
      "Epoch 332: 100%|██████████| 79/79 [00:00<00:00, 153.66batch/s, acc=0.957, loss=0.0671]\n",
      "Epoch 333: 100%|██████████| 79/79 [00:00<00:00, 142.00batch/s, acc=0.957, loss=0.0979]\n",
      "Epoch 334: 100%|██████████| 79/79 [00:00<00:00, 148.09batch/s, acc=0.957, loss=0.0653]\n",
      "Epoch 335: 100%|██████████| 79/79 [00:00<00:00, 155.54batch/s, acc=1, loss=0.0876]  \n",
      "Epoch 336: 100%|██████████| 79/79 [00:00<00:00, 150.27batch/s, acc=0.957, loss=0.0591]\n",
      "Epoch 337: 100%|██████████| 79/79 [00:00<00:00, 150.83batch/s, acc=1, loss=0.0541]   \n",
      "Epoch 338: 100%|██████████| 79/79 [00:00<00:00, 153.98batch/s, acc=1, loss=0.049]    \n",
      "Epoch 339: 100%|██████████| 79/79 [00:00<00:00, 143.59batch/s, acc=0.913, loss=0.165]\n",
      "Epoch 340: 100%|██████████| 79/79 [00:00<00:00, 151.56batch/s, acc=0.957, loss=0.115]\n",
      "Epoch 341: 100%|██████████| 79/79 [00:00<00:00, 145.46batch/s, acc=1, loss=0.072]    \n",
      "Epoch 342: 100%|██████████| 79/79 [00:00<00:00, 150.84batch/s, acc=1, loss=0.0471]   \n",
      "Epoch 343: 100%|██████████| 79/79 [00:00<00:00, 149.94batch/s, acc=0.957, loss=0.0809]\n",
      "Epoch 344: 100%|██████████| 79/79 [00:00<00:00, 144.81batch/s, acc=0.957, loss=0.098]\n",
      "Epoch 345: 100%|██████████| 79/79 [00:00<00:00, 144.91batch/s, acc=1, loss=0.0595]   \n",
      "Epoch 346: 100%|██████████| 79/79 [00:00<00:00, 142.70batch/s, acc=1, loss=0.0415]   \n",
      "Epoch 347: 100%|██████████| 79/79 [00:00<00:00, 142.92batch/s, acc=1, loss=0.0527]   \n",
      "Epoch 348: 100%|██████████| 79/79 [00:00<00:00, 155.17batch/s, acc=0.957, loss=0.055]\n",
      "Epoch 349: 100%|██████████| 79/79 [00:00<00:00, 148.00batch/s, acc=1, loss=0.0338]   \n",
      "Epoch 350: 100%|██████████| 79/79 [00:00<00:00, 153.07batch/s, acc=1, loss=0.0421]   \n",
      "Epoch 351: 100%|██████████| 79/79 [00:00<00:00, 152.46batch/s, acc=1, loss=0.06]     \n",
      "Epoch 352: 100%|██████████| 79/79 [00:00<00:00, 151.28batch/s, acc=1, loss=0.0658]   \n",
      "Epoch 353: 100%|██████████| 79/79 [00:00<00:00, 145.96batch/s, acc=0.957, loss=0.13] \n",
      "Epoch 354: 100%|██████████| 79/79 [00:00<00:00, 144.68batch/s, acc=1, loss=0.0568]   \n",
      "Epoch 355: 100%|██████████| 79/79 [00:00<00:00, 151.02batch/s, acc=1, loss=0.0692]   \n",
      "Epoch 356: 100%|██████████| 79/79 [00:00<00:00, 144.34batch/s, acc=1, loss=0.052]    \n",
      "Epoch 357: 100%|██████████| 79/79 [00:00<00:00, 143.93batch/s, acc=0.957, loss=0.0497]\n",
      "Epoch 358:  82%|████████▏ | 65/79 [00:00<00:00, 147.97batch/s, acc=0.89, loss=0.215] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yvesi\\Documents\\GitHub\\kaggle-notebooks\\Spaceship_Titanic\\classifier.ipynb Zelle 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yvesi/Documents/GitHub/kaggle-notebooks/Spaceship_Titanic/classifier.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_train(model, X_train_t, y_train_t, X_val_t, y_val_t)\n",
      "\u001b[1;32mc:\\Users\\yvesi\\Documents\\GitHub\\kaggle-notebooks\\Spaceship_Titanic\\classifier.ipynb Zelle 9\u001b[0m in \u001b[0;36mmodel_train\u001b[1;34m(model, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yvesi/Documents/GitHub/kaggle-notebooks/Spaceship_Titanic/classifier.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m y_batch \u001b[39m=\u001b[39m y_train[start:start\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yvesi/Documents/GitHub/kaggle-notebooks/Spaceship_Titanic/classifier.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yvesi/Documents/GitHub/kaggle-notebooks/Spaceship_Titanic/classifier.ipynb#X42sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(X_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yvesi/Documents/GitHub/kaggle-notebooks/Spaceship_Titanic/classifier.ipynb#X42sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yvesi/Documents/GitHub/kaggle-notebooks/Spaceship_Titanic/classifier.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yvesi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1104\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m-> 1104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1105\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[0;32m   1106\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_train(model, X_train_t, y_train_t, X_val_t, y_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0018_01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0019_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0021_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0023_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>9266_02</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>9269_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>9271_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>9273_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>9277_01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4277 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Transported\n",
       "0        0013_01         True\n",
       "1        0018_01        False\n",
       "2        0019_01         True\n",
       "3        0021_01         True\n",
       "4        0023_01         True\n",
       "...          ...          ...\n",
       "4272     9266_02         True\n",
       "4273     9269_01         True\n",
       "4274     9271_01         True\n",
       "4275     9273_01         True\n",
       "4276     9277_01        False\n",
       "\n",
       "[4277 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t = torch.tensor(preprocess(df_test).values.astype(np.float32)).to(device)\n",
    "model.eval()\n",
    "pred_t = model(X_train_t)\n",
    "pred = pd.DataFrame(pred_t.detach().numpy(), columns=['Transported'])\n",
    "pred['Transported'] = pred['Transported'].apply(np.round).astype(bool)\n",
    "result = pd.concat([df_test['PassengerId'], pred], axis=1)\n",
    "result.to_csv('prediction.csv', index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b5c5edfa0cdb88b5fff1ce8ac6bd5461652e2ada32bb50d564afab8b7a1fdfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
